# NUWAâ€‘LIP ğŸš€

**NUWAâ€‘LIP** is a stateâ€‘ofâ€‘theâ€‘art, multiâ€‘modal framework for masked imageâ€‘text modeling and conditional image generation.  
Powered by CLIP, VQGAN, and Megatronâ€‘LR, it supports a suite of pretraining and finetuning pipelines on MSCOCO, Conceptual Captions, custom datasets, and more.

---

## ğŸ”¥ Key Features

- **Masked Visionâ€‘Language Modeling**  
  â€£ Generate and apply spatial/textual masks with `MaskingGenerator`  
  â€£ Support for VMLM (_Vision-Masked Language Modeling_) & MLM  
- **Conditional Image Generation**  
  â€£ Leverage a pretrained painter network _(mps4coco / lip4maskcoco / lip4custom)_  
  â€£ Seamless integration with DFâ€‘VQGAN for discrete visual tokens  
- **Scalable Training & Inference**  
  â€£ Singleâ€‘node or distributed via PyTorch DDP / Megatron  
  â€£ Customizable pipelines: `run.sh`, `finetune.py`, `collector/` hooks  
- **Utility Arsenal**  
  â€£ `wutils.py` for I/O, logging, checkpoints, LMDB, SFTP, JSON/TSV handling  
  â€£ Builtâ€‘in DataLoaderX, Trainer, Meter, and more  

---

## ğŸ—‚ Repository Layout

```
NUWAâ€‘LIP/
â”œâ”€â”€ config/            # Model configurations & dataset definitions
â”‚   â”œâ”€ mps4coco/       # MVLM + conditional painter
â”‚   â”œâ”€ lip4maskcoco/   # Lipâ€‘based masked modeling on COCO
â”‚   â”œâ”€ lip4custom/     # Custom downstream tasks
â”‚   â””â”€ dfvqgan.py      # DFâ€‘VQGAN module & codebook interfaces
â”œâ”€â”€ collector/         # Hooks for inference & training collectors
â”œâ”€â”€ wutils.py          # Utilities: I/O, trainers, checkpointing, SFTP, etc.
â”œâ”€â”€ finetune.py        # Entry point for training & evaluation
â”œâ”€â”€ run.sh             # Platformâ€‘agnostic launch script
â””â”€â”€ README.md          
```

---

## âš™ï¸ Installation

1. Clone this repo:  
   ```bash
   git clone https://github.com/your-org/NUWA-LIP.git
   cd NUWA-LIP
   ```
2. Create & activate your environment (Conda/Pipenv/venv):  
   ```bash
   pip install -r requirements.txt
   ```
3. Download pretrained CLIP & VQGAN weights:  
   ```bash
   # ensure CLIP/VQGAN files under $ROOT/CLIP and $ROOT/checkpoint/DFâ€‘VQGAN/
   bash scripts/download_pretrained.sh
   ```

---

## ğŸš€ Quick Start

### 1. Pretraining on MSCOCO (MVLM)
```bash
bash run.sh \
  --config config/mps4coco/base.py \
  --action train \
  --platform local \
  --train_local_batch_size 4 \
  --eval_local_batch_size 1
```

### 2. Finetuning for Masked Text Generation
```bash
python finetune.py \
  --config config/lip4maskcoco/base.py \
  --do_train \
  --dist False \
  --local_train_batch_size 8
```

### 3. Inference & Visualization
```bash
python finetune.py \
  --config config/lip4custom/base.py \
  --do_eval_visu \
  --visu_split val
```

---

## ğŸ“– Configuration

All hyperparameters and file paths are exposed via `Args` classes in `config/*/base.py`.  
Easily override:
```python
args = Args()
args.learning_rate = 3e-4
args.epochs = 1000
```

---

## â­ï¸ Citation

If you use **NUWAâ€‘LIP** in your research, please cite:

```
@inproceedings{NUWA-LIP2023,
  title={NUWAâ€‘LIP: Unified Largeâ€‘scale Image-Text Pretraining},
  author={Your Name and Collaborators},
  year={2023},
  booktitle={Conference/Journal}
}
```
