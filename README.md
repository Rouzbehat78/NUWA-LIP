
# NUWAâ€‘LIP ğŸš€  
_A Next-Generation Framework for Language-Guided Image Inpainting and Generation_

---

## ğŸŒŸ Overview

**NUWAâ€‘LIP** is a cutting-edge, multi-modal framework designed for masked image-text modeling and conditional image generation.  
It seamlessly integrates large language-vision models (like CLIP), discrete visual token models (like DF-VQGAN), and advanced transformer architectures (like Megatron-LR) to deliver high-quality, scalable, and customizable visual generation pipelines.

Whether youâ€™re working on text-conditioned inpainting, spatially masked reconstruction, or fine-tuned creative generation on custom datasets, NUWA-LIP provides an efficient, modular, and research-ready foundation.

---

## ğŸ”‘ Core Contributions

âœ… **Unified Masked Image-Text Framework**  
â†’ Supports both Vision-Masked Language Modeling (VMLM) and standard Masked Language Modeling (MLM).

âœ… **Advanced Visual Tokenization**  
â†’ Uses DFâ€‘VQGAN to discretize visual spaces, making transformer modeling over images efficient.

âœ… **Scalable, Modular Training**  
â†’ Built-in support for single-node and distributed PyTorch DDP; easy-to-extend configs and hooks.

âœ… **Custom Dataset Integration**  
â†’ Pre-configured pipelines for MSCOCO, Conceptual Captions, and easy extension to your own datasets.

âœ… **Plug-and-Play Decoder Options**  
â†’ Flexible second-stage decoders (MP-S2S, Mamba, MaskGIT, etc.) for balancing quality vs. inference speed.

âœ… **Rich Utility Layer**  
â†’ I/O, logging, checkpoints, LMDB management, SFTP syncing, and custom data handling all built-in.

---

## ğŸ“¦ Repository Structure

```
NUWA-LIP/
â”œâ”€â”€ aim/                  # Mamba-based fast transformer modules
â”œâ”€â”€ collector/            # MP-S2S inference/training components
â”œâ”€â”€ config/               # Configurations for various datasets and pipelines
â”œâ”€â”€ wutils.py             # Utilities for logging, file ops, and data loading
â”œâ”€â”€ finetune.py           # Fine-tuning pipeline
â”œâ”€â”€ run.sh                # Sample shell script for execution
â”œâ”€â”€ README.md             # This file!
â””â”€â”€ requirements.txt      # Python dependencies
```

---

## âš™ï¸ Installation

```bash
git clone https://github.com/Rouzbehat78/NUWA-LIP.git
cd NUWA-LIP
pip install -r requirements.txt
```

Optional (for GPU acceleration):
```bash
pip install torch torchvision --extra-index-url https://download.pytorch.org/whl/cu118
```

---

## ğŸš€ Usage

### Train or Fine-Tune
```bash
bash run.sh --config config/lip4maskcoco/base.py --train
```

### Run Inference
```bash
python collector/mps_inference.py --input your_input_image.jpg --prompt "A sunset over the mountains"
```

### Replace Decoder with Faster Mamba Variant
```bash
python aim/mamba_training.py --config config/your_config.py
```

---

## ğŸ“Š Results

| **Model Variant**         | **Min Val Loss â†“** | **Final Loss â†“** | **Latency (ms) â†“** |
|---------------------------|---------------------|-------------------|---------------------|
| NUWA-LIP Original         | N/A                 | N/A               | 204.7               |
| NUWA-LIP w/ Mamba (Ours)  | **0.89**           | 0.91              | **73.9**            |
| Ablation â€“ No Refinement  | 2.47                | 2.50              | **0.0**             |

> ğŸ’¡ **Key takeaway:** Our Mamba-based decoder achieves a ~3x speedup with minimal performance degradation.

---

## ğŸ¤ Contributions
TODO
